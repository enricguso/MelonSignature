{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GTZAN and MusiCNN-MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import essentia.standard as es\n",
    "from essentia import Pool\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "###############################################################################################\n",
    "\n",
    "# Please first specify the path of your GTZAN dataset if it is already downloaded in your system.\n",
    "    # Otherwise leave 'data' or the desired path where we will download the dataset.\n",
    "\n",
    "GTZAN_path = 'data'\n",
    "#GTZAN_path = <your_path>\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# TensorflowPredictMusiCNN expects mono 16kHz sample rate inputs. Resample needed\n",
    "resample = es.Resample(inputSampleRate=22050, outputSampleRate=16000, quality=0)\n",
    "\n",
    "# Download dataset from torchaudio\n",
    "if not os.path.isdir(GTZAN_path):\n",
    "    os.mkdir(GTZAN_path)\n",
    "    train_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, download=True, subset='training')\n",
    "else:\n",
    "    train_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='training')\n",
    "val_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='validation')\n",
    "test_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='testing')\n",
    "\n",
    "# We download the essentia MSD MusiCNN model\n",
    "if not os.path.isfile('msd-musicnn-1.pb'):\n",
    "    !curl -SLO https://essentia.upf.edu/models/autotagging/msd/msd-musicnn-1.pb\n",
    "\n",
    "class Essentia_MusiCNNMSD_GTZAN_Dataset(Dataset):\n",
    "    \"\"\" The embeddings of the GTZAN dataset extracted with Essentia-Tensorflow's MusiCNN-MSD model. \"\"\"\n",
    "    def __init__(self, GTZAN_dataset, embeddings):\n",
    "        self.GTZAN_dataset = GTZAN_dataset\n",
    "        self.embeddings = embeddings\n",
    "        self.GTZAN_genres = [\n",
    "            \"blues\",\n",
    "            \"classical\",\n",
    "            \"country\",\n",
    "            \"disco\",\n",
    "            \"hiphop\",\n",
    "            \"jazz\",\n",
    "            \"metal\",\n",
    "            \"pop\",\n",
    "            \"reggae\",\n",
    "            \"rock\",\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.GTZAN_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        inputs = torch.from_numpy(self.embeddings[idx]).mean(0) #comment mean for original method\n",
    "        labels = torch.tensor(self.GTZAN_genres.index(self.GTZAN_dataset[idx][2]))\n",
    "        \n",
    "        return inputs, labels\n",
    "# We define a shallow model\n",
    "\n",
    "class shallowClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shallowClassifier, self).__init__()\n",
    "        self.dense1 = nn.Linear(200, 100) #change to 19*200 if commenting .mean()avobe\n",
    "        self.dense2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 200) #change to 200*19 if commenting .mean() above\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build embeddings with GTZAN time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        train_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings.npy',train_embeddings)\n",
    "\n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        val_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    val_embeddings=np.array(val_embeddings)    \n",
    "    np.save('val_embeddings.npy',val_embeddings)\n",
    "\n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        test_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    test_embeddings=np.array(test_embeddings)\n",
    "    np.save('test_embeddings.npy',np.array(test_embeddings))\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings.npy')\n",
    "    val_embeddings=np.load('val_embeddings.npy')\n",
    "    test_embeddings=np.load('test_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding shapes...\n",
    "train_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding types\n",
    "train_embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embtrain_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(train_dataset, train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_embeddings, val_embeddings, test_embeddings):\n",
    "    # We compute the distance of the embeddings between all songs in the training set\n",
    "    emb_distance = np.zeros((len(train_embeddings), len(train_embeddings)))\n",
    "\n",
    "    for indxA, trackA in enumerate(train_embeddings):\n",
    "        for indxB, trackB in enumerate(train_embeddings):\n",
    "            emb_distance[indxA, indxB] = np.linalg.norm(trackA - trackB)        \n",
    "\n",
    "    embtrain_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(train_dataset, train_embeddings)\n",
    "    train_loader = torch.utils.data.DataLoader(embtrain_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "    embval_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(val_dataset, val_embeddings)\n",
    "    val_loader = torch.utils.data.DataLoader(embval_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    embtest_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(test_dataset, test_embeddings)\n",
    "    test_loader = torch.utils.data.DataLoader(embtest_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = shallowClassifier()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "    num_epochs = 2000\n",
    "    train_losses = torch.zeros(num_epochs)\n",
    "    val_losses = torch.zeros(num_epochs)\n",
    "\n",
    "    bestloss = 100000.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #The train loop\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            # Send data to the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Clear gradient and forward + loss + backward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses[epoch] += loss.item()\n",
    "        train_losses[epoch] /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Send data to the GPU\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_losses[epoch] += loss.item()\n",
    "            val_losses[epoch] /= len(val_loader)\n",
    "            scheduler.step(val_losses[epoch])\n",
    "\n",
    "            # If best epoch, we save parameters\n",
    "            if val_losses[epoch] < bestloss :\n",
    "                bestloss = val_losses[epoch]\n",
    "                torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "        print('Epoch '+str(epoch)+': Train Loss = '+str(train_losses[epoch].item())+'. Val Loss = '+str(val_losses[epoch].item())+'.')\n",
    "    print('Best validation loss :' + str(bestloss.item()))\n",
    "\n",
    "    # Finally we compute accuracy with the test set\n",
    "    model.load_state_dict(torch.load('model.pth'));\n",
    "    model.eval()\n",
    "    confusion_matrix = torch.zeros(len(embtrain_dataset.GTZAN_genres), len(embtrain_dataset.GTZAN_genres))\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Send data to the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    # Per-class Accuracy\n",
    "    pclass_acc = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
    "    return torch.mean(pclass_acc).item()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GTZAN_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline is 80.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = resample(train_dataset[0][0].numpy()[0])\n",
    "original = es.TensorflowPredictMusiCNN(graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melspectrogram(audio):\n",
    "    # Computes the mel spectrogram of audio inputs as done in the MelonPlaylist dataset\n",
    "    windowing = es.Windowing(type='hann', normalized=False, zeroPadding=0)\n",
    "    spectrum = es.Spectrum()\n",
    "    melbands = es.MelBands(numberBands=48,\n",
    "                                   sampleRate=16000,\n",
    "                                   lowFrequencyBound=0,\n",
    "                                   highFrequencyBound=16000/2,\n",
    "                                   inputSize=(512+0)//2+1,\n",
    "                                   weighting='linear',\n",
    "                                   normalize='unit_tri',\n",
    "                                   warpingFormula='slaneyMel',\n",
    "                                   type='power')\n",
    "    amp2db = es.UnaryOperator(type='lin2db', scale=2)\n",
    "    result = []\n",
    "    for frame in es.FrameGenerator(audio, frameSize=512, hopSize=256,\n",
    "                                   startFromZero=False):\n",
    "        spectrumFrame = spectrum(windowing(frame))\n",
    "\n",
    "        melFrame = melbands(spectrumFrame)\n",
    "        result.append(amp2db(melFrame))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_melonInput_TensorflowPredict(melon_sample):\n",
    "    db2amp = es.UnaryOperator(type='db2lin', scale=2)\n",
    "    oversampled = np.zeros((len(melon_sample), melon_sample.shape[1]*2)).astype(np.float32)\n",
    "    for k in range(len(melon_sample)):\n",
    "        sample = np.log10(1 + (db2amp(melon_sample[k])*10000))\n",
    "        oversampled[k,:]=np.interp(np.arange(96)/2, np.arange(48), sample)\n",
    "    # Now we cut again, but with hop size of 93 frames as in default TensorflowPredictMusiCNN\n",
    "    new = np.zeros((int(len(oversampled) / 93) - 1, 187, 96)).astype(np.float32)\n",
    "    for k in range(int(len(oversampled) / 93) - 1):\n",
    "        new[k]=oversampled[k*93:k*93+187]\n",
    "    return np.expand_dims(new, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName='msd-musicnn-1.pb'\n",
    "output_layer='model/dense/BiasAdd'\n",
    "input_layer='model/Placeholder'\n",
    "predict = es.TensorflowPredict(graphFilename=modelName,\n",
    "                               inputs=[input_layer],\n",
    "                               outputs=[output_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pool = Pool()\n",
    "in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(audio)))\n",
    "output = predict(in_pool)\n",
    "prediction = output['model/dense/BiasAdd'][:,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorflowPredictMusiCNN expects mono 16kHz sample rate inputs. Resample needed\n",
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings_melon.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        train_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings_melon.npy', train_embeddings)\n",
    "    \n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        val_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    val_embeddings = np.array(val_embeddings)\n",
    "    np.save('val_embeddings_melon.npy', val_embeddings)\n",
    "    \n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        test_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    test_embeddings = np.array(test_embeddings)\n",
    "    np.save('test_embeddings_melon.npy', test_embeddings)    \n",
    "\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings_melon.npy')\n",
    "    val_embeddings=np.load('val_embeddings_melon.npy')\n",
    "    test_embeddings=np.load('test_embeddings_melon.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding shapes...\n",
    "train_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding types\n",
    "train_embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train, now with interpolated melspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = train_test(np.reshape(5*np.random.randn(train_embeddings.size).astype(np.float32), train_embeddings.shape), \n",
    "                 np.reshape(5*np.random.randn(val_embeddings.size).astype(np.float32), val_embeddings.shape), \n",
    "                 np.reshape(5*np.random.randn(test_embeddings.size).astype(np.float32), test_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with random musiCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_V(nn.Module):\n",
    "    # vertical convolution\n",
    "    def __init__(self, input_channels, output_channels, filter_shape):\n",
    "        super(Conv_V, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, filter_shape,\n",
    "                              padding=(0, filter_shape[1]//2))\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        freq = x.size(2)\n",
    "        out = nn.MaxPool2d((freq, 1), stride=(freq, 1))(x)\n",
    "        out = out.squeeze(2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv_H(nn.Module):\n",
    "    # horizontal convolution\n",
    "    def __init__(self, input_channels, output_channels, filter_length):\n",
    "        super(Conv_H, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, filter_length,\n",
    "                              padding=filter_length//2)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        freq = x.size(2)\n",
    "        out = nn.AvgPool2d((freq, 1), stride=(freq, 1))(x)\n",
    "        out = out.squeeze(2)\n",
    "        out = self.relu(self.bn(self.conv(out)))\n",
    "        return out\n",
    "\n",
    "class Conv_1d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, stride=1, pooling=2):\n",
    "        super(Conv_1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, shape, stride=stride, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool1d(pooling)\n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "    \n",
    "class Musicnn(nn.Module):\n",
    "    '''\n",
    "    Pons et al. 2017\n",
    "    End-to-end learning for music audio tagging at scale.\n",
    "    This is the updated implementation of the original paper. Referred to the Musicnn code.\n",
    "    https://github.com/jordipons/musicnn\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                sample_rate=16000,\n",
    "                n_fft=512,\n",
    "                f_min=0.0,\n",
    "                f_max=8000.0,\n",
    "                n_mels=96,\n",
    "                n_class=50,\n",
    "                dataset='mtat'):\n",
    "        super(Musicnn, self).__init__()\n",
    "\n",
    "        # Spectrogram\n",
    "        self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                         n_fft=n_fft,\n",
    "                                                         f_min=f_min,\n",
    "                                                         f_max=f_max,\n",
    "                                                         n_mels=n_mels)\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.spec_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # Pons front-end\n",
    "        m1 = Conv_V(1, 204, (int(0.7*96), 7))\n",
    "        m2 = Conv_V(1, 204, (int(0.4*96), 7))\n",
    "        m3 = Conv_H(1, 51, 129)\n",
    "        m4 = Conv_H(1, 51, 65)\n",
    "        m5 = Conv_H(1, 51, 33)\n",
    "        self.layers = nn.ModuleList([m1, m2, m3, m4, m5])\n",
    "\n",
    "        # Pons back-end\n",
    "        backend_channel= 512 if dataset=='msd' else 64\n",
    "        self.layer1 = Conv_1d(561, backend_channel, 7, 1, 1)\n",
    "        self.layer2 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "        self.layer3 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "\n",
    "        # Dense\n",
    "        dense_channel = 500 if dataset=='msd' else 200\n",
    "        self.dense1 = nn.Linear((561+(backend_channel*3))*2, dense_channel)\n",
    "        self.bn = nn.BatchNorm1d(dense_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(dense_channel, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Spectrogram\n",
    "        x = self.spec(x)\n",
    "        x = self.to_db(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.spec_bn(x)\n",
    "\n",
    "        # Pons front-end\n",
    "        out = []\n",
    "        for layer in self.layers:\n",
    "            out.append(layer(x))\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        # Pons back-end\n",
    "        length = out.size(2)\n",
    "        res1 = self.layer1(out)\n",
    "        res2 = self.layer2(res1) + res1\n",
    "        res3 = self.layer3(res2) + res2\n",
    "        out = torch.cat([out, res1, res2, res3], 1)\n",
    "\n",
    "        mp = nn.MaxPool1d(length)(out)\n",
    "        avgp = nn.AvgPool1d(length)(out)\n",
    "        out = torch.cat([mp, avgp], dim=1)\n",
    "        out = out.squeeze(2)\n",
    "\n",
    "        out = self.relu(self.bn(self.dense1(out)))\n",
    "        out = self.dropout(out)\n",
    "        #out = self.dense2(out)\n",
    "        #out = nn.Sigmoid()(out)\n",
    "\n",
    "        return out\n",
    "rand_musiCNN = Musicnn()\n",
    "rand_musiCNN.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing track 1 of 443\n",
      "Processing track 2 of 443\n",
      "Processing track 3 of 443\n",
      "Processing track 4 of 443\n",
      "Processing track 5 of 443\n",
      "Processing track 6 of 443\n",
      "Processing track 7 of 443\n",
      "Processing track 8 of 443\n",
      "Processing track 9 of 443\n",
      "Processing track 10 of 443\n",
      "Processing track 11 of 443\n",
      "Processing track 12 of 443\n",
      "Processing track 13 of 443\n",
      "Processing track 14 of 443\n",
      "Processing track 15 of 443\n",
      "Processing track 16 of 443\n",
      "Processing track 17 of 443\n",
      "Processing track 18 of 443\n",
      "Processing track 19 of 443\n",
      "Processing track 20 of 443\n",
      "Processing track 21 of 443\n",
      "Processing track 22 of 443\n",
      "Processing track 23 of 443\n",
      "Processing track 24 of 443\n",
      "Processing track 25 of 443\n",
      "Processing track 26 of 443\n",
      "Processing track 27 of 443\n",
      "Processing track 28 of 443\n",
      "Processing track 29 of 443\n",
      "Processing track 30 of 443\n",
      "Processing track 31 of 443\n",
      "Processing track 32 of 443\n",
      "Processing track 33 of 443\n",
      "Processing track 34 of 443\n",
      "Processing track 35 of 443\n",
      "Processing track 36 of 443\n",
      "Processing track 37 of 443\n",
      "Processing track 38 of 443\n",
      "Processing track 39 of 443\n",
      "Processing track 40 of 443\n",
      "Processing track 41 of 443\n",
      "Processing track 42 of 443\n",
      "Processing track 43 of 443\n",
      "Processing track 44 of 443\n",
      "Processing track 45 of 443\n",
      "Processing track 46 of 443\n",
      "Processing track 47 of 443\n",
      "Processing track 48 of 443\n",
      "Processing track 49 of 443\n",
      "Processing track 50 of 443\n",
      "Processing track 51 of 443\n",
      "Processing track 52 of 443\n",
      "Processing track 53 of 443\n",
      "Processing track 54 of 443\n",
      "Processing track 55 of 443\n",
      "Processing track 56 of 443\n",
      "Processing track 57 of 443\n",
      "Processing track 58 of 443\n",
      "Processing track 59 of 443\n",
      "Processing track 60 of 443\n",
      "Processing track 61 of 443\n",
      "Processing track 62 of 443\n",
      "Processing track 63 of 443\n",
      "Processing track 64 of 443\n",
      "Processing track 65 of 443\n",
      "Processing track 66 of 443\n",
      "Processing track 67 of 443\n",
      "Processing track 68 of 443\n",
      "Processing track 69 of 443\n",
      "Processing track 70 of 443\n",
      "Processing track 71 of 443\n",
      "Processing track 72 of 443\n",
      "Processing track 73 of 443\n",
      "Processing track 74 of 443\n",
      "Processing track 75 of 443\n",
      "Processing track 76 of 443\n",
      "Processing track 77 of 443\n",
      "Processing track 78 of 443\n",
      "Processing track 79 of 443\n",
      "Processing track 80 of 443\n",
      "Processing track 81 of 443\n",
      "Processing track 82 of 443\n",
      "Processing track 83 of 443\n",
      "Processing track 84 of 443\n",
      "Processing track 85 of 443\n",
      "Processing track 86 of 443\n",
      "Processing track 87 of 443\n",
      "Processing track 88 of 443\n",
      "Processing track 89 of 443\n",
      "Processing track 90 of 443\n",
      "Processing track 91 of 443\n",
      "Processing track 92 of 443\n",
      "Processing track 93 of 443\n",
      "Processing track 94 of 443\n",
      "Processing track 95 of 443\n",
      "Processing track 96 of 443\n",
      "Processing track 97 of 443\n",
      "Processing track 98 of 443\n",
      "Processing track 99 of 443\n",
      "Processing track 100 of 443\n",
      "Processing track 101 of 443\n",
      "Processing track 102 of 443\n",
      "Processing track 103 of 443\n",
      "Processing track 104 of 443\n",
      "Processing track 105 of 443\n",
      "Processing track 106 of 443\n",
      "Processing track 107 of 443\n",
      "Processing track 108 of 443\n",
      "Processing track 109 of 443\n",
      "Processing track 110 of 443\n",
      "Processing track 111 of 443\n",
      "Processing track 112 of 443\n",
      "Processing track 113 of 443\n",
      "Processing track 114 of 443\n",
      "Processing track 115 of 443\n",
      "Processing track 116 of 443\n",
      "Processing track 117 of 443\n",
      "Processing track 118 of 443\n",
      "Processing track 119 of 443\n",
      "Processing track 120 of 443\n",
      "Processing track 121 of 443\n",
      "Processing track 122 of 443\n",
      "Processing track 123 of 443\n",
      "Processing track 124 of 443\n",
      "Processing track 125 of 443\n",
      "Processing track 126 of 443\n",
      "Processing track 127 of 443\n",
      "Processing track 128 of 443\n",
      "Processing track 129 of 443\n",
      "Processing track 130 of 443\n",
      "Processing track 131 of 443\n",
      "Processing track 132 of 443\n",
      "Processing track 133 of 443\n",
      "Processing track 134 of 443\n",
      "Processing track 135 of 443\n",
      "Processing track 136 of 443\n",
      "Processing track 137 of 443\n",
      "Processing track 138 of 443\n",
      "Processing track 139 of 443\n",
      "Processing track 140 of 443\n",
      "Processing track 141 of 443\n",
      "Processing track 142 of 443\n",
      "Processing track 143 of 443\n",
      "Processing track 144 of 443\n",
      "Processing track 145 of 443\n",
      "Processing track 146 of 443\n",
      "Processing track 147 of 443\n",
      "Processing track 148 of 443\n",
      "Processing track 149 of 443\n",
      "Processing track 150 of 443\n",
      "Processing track 151 of 443\n",
      "Processing track 152 of 443\n",
      "Processing track 153 of 443\n",
      "Processing track 154 of 443\n",
      "Processing track 155 of 443\n",
      "Processing track 156 of 443\n",
      "Processing track 157 of 443\n",
      "Processing track 158 of 443\n",
      "Processing track 159 of 443\n",
      "Processing track 160 of 443\n",
      "Processing track 161 of 443\n",
      "Processing track 162 of 443\n",
      "Processing track 163 of 443\n",
      "Processing track 164 of 443\n",
      "Processing track 165 of 443\n",
      "Processing track 166 of 443\n",
      "Processing track 167 of 443\n",
      "Processing track 168 of 443\n",
      "Processing track 169 of 443\n",
      "Processing track 170 of 443\n",
      "Processing track 171 of 443\n",
      "Processing track 172 of 443\n",
      "Processing track 173 of 443\n",
      "Processing track 174 of 443\n",
      "Processing track 175 of 443\n",
      "Processing track 176 of 443\n",
      "Processing track 177 of 443\n",
      "Processing track 178 of 443\n",
      "Processing track 179 of 443\n",
      "Processing track 180 of 443\n",
      "Processing track 181 of 443\n",
      "Processing track 182 of 443\n",
      "Processing track 183 of 443\n",
      "Processing track 184 of 443\n",
      "Processing track 185 of 443\n",
      "Processing track 186 of 443\n",
      "Processing track 187 of 443\n",
      "Processing track 188 of 443\n",
      "Processing track 189 of 443\n",
      "Processing track 190 of 443\n",
      "Processing track 191 of 443\n",
      "Processing track 192 of 443\n",
      "Processing track 193 of 443\n",
      "Processing track 194 of 443\n",
      "Processing track 195 of 443\n",
      "Processing track 196 of 443\n",
      "Processing track 197 of 443\n",
      "Processing track 198 of 443\n",
      "Processing track 199 of 443\n",
      "Processing track 200 of 443\n",
      "Processing track 201 of 443\n",
      "Processing track 202 of 443\n",
      "Processing track 203 of 443\n",
      "Processing track 204 of 443\n",
      "Processing track 205 of 443\n",
      "Processing track 206 of 443\n",
      "Processing track 207 of 443\n",
      "Processing track 208 of 443\n",
      "Processing track 209 of 443\n",
      "Processing track 210 of 443\n",
      "Processing track 211 of 443\n",
      "Processing track 212 of 443\n",
      "Processing track 213 of 443\n",
      "Processing track 214 of 443\n",
      "Processing track 215 of 443\n",
      "Processing track 216 of 443\n",
      "Processing track 217 of 443\n",
      "Processing track 218 of 443\n",
      "Processing track 219 of 443\n",
      "Processing track 220 of 443\n",
      "Processing track 221 of 443\n",
      "Processing track 222 of 443\n",
      "Processing track 223 of 443\n",
      "Processing track 224 of 443\n",
      "Processing track 225 of 443\n",
      "Processing track 226 of 443\n",
      "Processing track 227 of 443\n",
      "Processing track 228 of 443\n",
      "Processing track 229 of 443\n",
      "Processing track 230 of 443\n",
      "Processing track 231 of 443\n",
      "Processing track 232 of 443\n",
      "Processing track 233 of 443\n",
      "Processing track 234 of 443\n",
      "Processing track 235 of 443\n",
      "Processing track 236 of 443\n",
      "Processing track 237 of 443\n",
      "Processing track 238 of 443\n",
      "Processing track 239 of 443\n",
      "Processing track 240 of 443\n",
      "Processing track 241 of 443\n",
      "Processing track 242 of 443\n",
      "Processing track 243 of 443\n",
      "Processing track 244 of 443\n",
      "Processing track 245 of 443\n",
      "Processing track 246 of 443\n",
      "Processing track 247 of 443\n",
      "Processing track 248 of 443\n",
      "Processing track 249 of 443\n",
      "Processing track 250 of 443\n",
      "Processing track 251 of 443\n",
      "Processing track 252 of 443\n",
      "Processing track 253 of 443\n",
      "Processing track 254 of 443\n",
      "Processing track 255 of 443\n",
      "Processing track 256 of 443\n",
      "Processing track 257 of 443\n",
      "Processing track 258 of 443\n",
      "Processing track 259 of 443\n",
      "Processing track 260 of 443\n",
      "Processing track 261 of 443\n",
      "Processing track 262 of 443\n",
      "Processing track 263 of 443\n",
      "Processing track 264 of 443\n",
      "Processing track 265 of 443\n",
      "Processing track 266 of 443\n",
      "Processing track 267 of 443\n",
      "Processing track 268 of 443\n",
      "Processing track 269 of 443\n",
      "Processing track 270 of 443\n",
      "Processing track 271 of 443\n",
      "Processing track 272 of 443\n",
      "Processing track 273 of 443\n",
      "Processing track 274 of 443\n",
      "Processing track 275 of 443\n",
      "Processing track 276 of 443\n"
     ]
    }
   ],
   "source": [
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings_random.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        train_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings_random.npy', train_embeddings)\n",
    "    \n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        val_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    val_embeddings = np.array(val_embeddings)\n",
    "    np.save('val_embeddings_random.npy', val_embeddings)\n",
    "    \n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        test_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    test_embeddings = np.array(test_embeddings)\n",
    "    np.save('test_embeddings_random.npy', test_embeddings)    \n",
    "\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings_random.npy')\n",
    "    val_embeddings=np.load('val_embeddings_random.npy')\n",
    "    test_embeddings=np.load('test_embeddings_random.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_net = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model        | Loss           | Accuracy  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Random embeddings      | 2.31 | 8.07% |\n",
    "| Random musiCNN      | ? | ? |\n",
    "| musiCNN time      |    0.67   |   80.50% |\n",
    "| musiCNN melon | 0.92      |    74.57% |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melon_venv",
   "language": "python",
   "name": "melon_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
