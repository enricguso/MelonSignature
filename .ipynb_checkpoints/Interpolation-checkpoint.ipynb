{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GTZAN and MusiCNN-MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import essentia.standard as es\n",
    "from essentia import Pool\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "###############################################################################################\n",
    "\n",
    "# Please first specify the path of your GTZAN dataset if it is already downloaded in your system.\n",
    "    # Otherwise leave 'data' or the desired path where we will download the dataset.\n",
    "\n",
    "GTZAN_path = 'data'\n",
    "#GTZAN_path = <your_path>\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# TensorflowPredictMusiCNN expects mono 16kHz sample rate inputs. Resample needed\n",
    "resample = es.Resample(inputSampleRate=22050, outputSampleRate=16000, quality=0)\n",
    "\n",
    "# Download dataset from torchaudio\n",
    "if not os.path.isdir(GTZAN_path):\n",
    "    os.mkdir(GTZAN_path)\n",
    "    train_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, download=True, subset='training')\n",
    "else:\n",
    "    train_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='training')\n",
    "val_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='validation')\n",
    "test_dataset = torchaudio.datasets.GTZAN(root=GTZAN_path, subset='testing')\n",
    "\n",
    "# We download the essentia MSD MusiCNN model\n",
    "if not os.path.isfile('msd-musicnn-1.pb'):\n",
    "    !curl -SLO https://essentia.upf.edu/models/autotagging/msd/msd-musicnn-1.pb\n",
    "\n",
    "class Essentia_MusiCNNMSD_GTZAN_Dataset(Dataset):\n",
    "    \"\"\" The embeddings of the GTZAN dataset extracted with Essentia-Tensorflow's MusiCNN-MSD model. \"\"\"\n",
    "    def __init__(self, GTZAN_dataset, embeddings):\n",
    "        self.GTZAN_dataset = GTZAN_dataset\n",
    "        self.embeddings = embeddings\n",
    "        self.GTZAN_genres = [\n",
    "            \"blues\",\n",
    "            \"classical\",\n",
    "            \"country\",\n",
    "            \"disco\",\n",
    "            \"hiphop\",\n",
    "            \"jazz\",\n",
    "            \"metal\",\n",
    "            \"pop\",\n",
    "            \"reggae\",\n",
    "            \"rock\",\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.GTZAN_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        inputs = torch.from_numpy(self.embeddings[idx]).mean(0) #comment mean for original method\n",
    "        labels = torch.tensor(self.GTZAN_genres.index(self.GTZAN_dataset[idx][2]))\n",
    "        \n",
    "        return inputs, labels\n",
    "# We define a shallow model\n",
    "\n",
    "class shallowClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(shallowClassifier, self).__init__()\n",
    "        self.dense1 = nn.Linear(200, 100) #change to 19*200 if commenting .mean()avobe\n",
    "        self.dense2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 200) #change to 200*19 if commenting .mean() above\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build embeddings with GTZAN time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        if i%100==0:\n",
    "            i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        train_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings.npy',train_embeddings)\n",
    "\n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        val_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    val_embeddings=np.array(val_embeddings)    \n",
    "    np.save('val_embeddings.npy',val_embeddings)\n",
    "\n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        test_embeddings.append(es.TensorflowPredictMusiCNN(\n",
    "            graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(resample(track[0].numpy()[0])))\n",
    "    test_embeddings=np.array(test_embeddings)\n",
    "    np.save('test_embeddings.npy',np.array(test_embeddings))\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings.npy')\n",
    "    val_embeddings=np.load('val_embeddings.npy')\n",
    "    test_embeddings=np.load('test_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding shapes...\n",
    "train_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding types\n",
    "train_embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embtrain_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(train_dataset, train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_embeddings, val_embeddings, test_embeddings):\n",
    "    # We compute the distance of the embeddings between all songs in the training set\n",
    "    emb_distance = np.zeros((len(train_embeddings), len(train_embeddings)))\n",
    "\n",
    "    for indxA, trackA in enumerate(train_embeddings):\n",
    "        for indxB, trackB in enumerate(train_embeddings):\n",
    "            emb_distance[indxA, indxB] = np.linalg.norm(trackA - trackB)        \n",
    "\n",
    "    embtrain_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(train_dataset, train_embeddings)\n",
    "    train_loader = torch.utils.data.DataLoader(embtrain_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "    embval_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(val_dataset, val_embeddings)\n",
    "    val_loader = torch.utils.data.DataLoader(embval_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    embtest_dataset = Essentia_MusiCNNMSD_GTZAN_Dataset(test_dataset, test_embeddings)\n",
    "    test_loader = torch.utils.data.DataLoader(embtest_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = shallowClassifier()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "    num_epochs = 2000\n",
    "    train_losses = torch.zeros(num_epochs)\n",
    "    val_losses = torch.zeros(num_epochs)\n",
    "\n",
    "    bestloss = 100000.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #The train loop\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            # Send data to the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Clear gradient and forward + loss + backward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses[epoch] += loss.item()\n",
    "        train_losses[epoch] /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Send data to the GPU\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_losses[epoch] += loss.item()\n",
    "            val_losses[epoch] /= len(val_loader)\n",
    "            scheduler.step(val_losses[epoch])\n",
    "\n",
    "            # If best epoch, we save parameters\n",
    "            if val_losses[epoch] < bestloss :\n",
    "                bestloss = val_losses[epoch]\n",
    "                torch.save(model.state_dict(), 'model.pth')\n",
    "                \n",
    "        if epoch%100==0:\n",
    "            print('Epoch '+str(epoch)+': Train Loss = '+str(train_losses[epoch].item())+'. Val Loss = '+str(val_losses[epoch].item())+'.')\n",
    "    print('Best validation loss :' + str(bestloss.item()))\n",
    "\n",
    "    # Finally we compute accuracy with the test set\n",
    "    model.load_state_dict(torch.load('model.pth'));\n",
    "    model.eval()\n",
    "    confusion_matrix = torch.zeros(len(embtrain_dataset.GTZAN_genres), len(embtrain_dataset.GTZAN_genres))\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Send data to the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    # Per-class Accuracy\n",
    "    pclass_acc = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
    "    return torch.mean(pclass_acc).item()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GTZAN_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 2.396742582321167. Val Loss = 2.302741050720215.\n",
      "Epoch 100: Train Loss = 1.0398520231246948. Val Loss = 1.171513557434082.\n",
      "Epoch 200: Train Loss = 0.6912970542907715. Val Loss = 0.885692298412323.\n",
      "Epoch 300: Train Loss = 0.5544649362564087. Val Loss = 0.7747247219085693.\n",
      "Epoch 400: Train Loss = 0.47854354977607727. Val Loss = 0.7215855717658997.\n",
      "Epoch 500: Train Loss = 0.4272844195365906. Val Loss = 0.6938523054122925.\n",
      "Epoch 600: Train Loss = 0.3927702009677887. Val Loss = 0.6783539056777954.\n",
      "Epoch 700: Train Loss = 0.3665103316307068. Val Loss = 0.6705629825592041.\n",
      "Epoch 800: Train Loss = 0.34445664286613464. Val Loss = 0.667378842830658.\n",
      "Epoch 900: Train Loss = 0.3370794653892517. Val Loss = 0.6668924689292908.\n",
      "Epoch 1000: Train Loss = 0.334261029958725. Val Loss = 0.6668926477432251.\n",
      "Epoch 1100: Train Loss = 0.33406737446784973. Val Loss = 0.6668928861618042.\n",
      "Epoch 1200: Train Loss = 0.3340103328227997. Val Loss = 0.6668930053710938.\n",
      "Epoch 1300: Train Loss = 0.33581289649009705. Val Loss = 0.6668932437896729.\n",
      "Epoch 1400: Train Loss = 0.3392503261566162. Val Loss = 0.6668933629989624.\n",
      "Epoch 1500: Train Loss = 0.3355485498905182. Val Loss = 0.6668933629989624.\n",
      "Epoch 1600: Train Loss = 0.3378586173057556. Val Loss = 0.6668936610221863.\n",
      "Epoch 1700: Train Loss = 0.33655914664268494. Val Loss = 0.6668938398361206.\n",
      "Epoch 1800: Train Loss = 0.3341066539287567. Val Loss = 0.6668939590454102.\n",
      "Epoch 1900: Train Loss = 0.3361029624938965. Val Loss = 0.6668940782546997.\n",
      "Best validation loss :0.66688472032547\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b6c1a273ea3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-070e0418adb4>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(train_embeddings, val_embeddings)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTZAN_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGTZAN_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Send data to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "baseline = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline is 80.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = resample(train_dataset[0][0].numpy()[0])\n",
    "original = es.TensorflowPredictMusiCNN(graphFilename='msd-musicnn-1.pb', output='model/dense/BiasAdd')(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melspectrogram(audio):\n",
    "    # Computes the mel spectrogram of audio inputs as done in the MelonPlaylist dataset\n",
    "    windowing = es.Windowing(type='hann', normalized=False, zeroPadding=0)\n",
    "    spectrum = es.Spectrum()\n",
    "    melbands = es.MelBands(numberBands=48,\n",
    "                                   sampleRate=16000,\n",
    "                                   lowFrequencyBound=0,\n",
    "                                   highFrequencyBound=16000/2,\n",
    "                                   inputSize=(512+0)//2+1,\n",
    "                                   weighting='linear',\n",
    "                                   normalize='unit_tri',\n",
    "                                   warpingFormula='slaneyMel',\n",
    "                                   type='power')\n",
    "    amp2db = es.UnaryOperator(type='lin2db', scale=2)\n",
    "    result = []\n",
    "    for frame in es.FrameGenerator(audio, frameSize=512, hopSize=256,\n",
    "                                   startFromZero=False):\n",
    "        spectrumFrame = spectrum(windowing(frame))\n",
    "\n",
    "        melFrame = melbands(spectrumFrame)\n",
    "        result.append(amp2db(melFrame))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_melonInput_TensorflowPredict(melon_sample):\n",
    "    db2amp = es.UnaryOperator(type='db2lin', scale=2)\n",
    "    oversampled = np.zeros((len(melon_sample), melon_sample.shape[1]*2)).astype(np.float32)\n",
    "    for k in range(len(melon_sample)):\n",
    "        sample = np.log10(1 + (db2amp(melon_sample[k])*10000))\n",
    "        oversampled[k,:]=np.interp(np.arange(96)/2, np.arange(48), sample)\n",
    "    # Now we cut again, but with hop size of 93 frames as in default TensorflowPredictMusiCNN\n",
    "    new = np.zeros((int(len(oversampled) / 93) - 1, 187, 96)).astype(np.float32)\n",
    "    for k in range(int(len(oversampled) / 93) - 1):\n",
    "        new[k]=oversampled[k*93:k*93+187]\n",
    "    return np.expand_dims(new, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName='msd-musicnn-1.pb'\n",
    "output_layer='model/dense/BiasAdd'\n",
    "input_layer='model/Placeholder'\n",
    "predict = es.TensorflowPredict(graphFilename=modelName,\n",
    "                               inputs=[input_layer],\n",
    "                               outputs=[output_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pool = Pool()\n",
    "in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(audio)))\n",
    "output = predict(in_pool)\n",
    "prediction = output['model/dense/BiasAdd'][:,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorflowPredictMusiCNN expects mono 16kHz sample rate inputs. Resample needed\n",
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings_melon.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        if i%100==0:\n",
    "            i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        train_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings_melon.npy', train_embeddings)\n",
    "    \n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        val_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    val_embeddings = np.array(val_embeddings)\n",
    "    np.save('val_embeddings_melon.npy', val_embeddings)\n",
    "    \n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        in_pool = Pool()\n",
    "        in_pool.set('model/Placeholder', adapt_melonInput_TensorflowPredict(melspectrogram(resample(track[0].numpy()[0]))))\n",
    "        output = predict(in_pool)\n",
    "        test_embeddings.append(output['model/dense/BiasAdd'][:,0,0,:])\n",
    "    test_embeddings = np.array(test_embeddings)\n",
    "    np.save('test_embeddings_melon.npy', test_embeddings)    \n",
    "\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings_melon.npy')\n",
    "    val_embeddings=np.load('val_embeddings_melon.npy')\n",
    "    test_embeddings=np.load('test_embeddings_melon.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding shapes...\n",
    "train_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding types\n",
    "train_embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train, now with interpolated melspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = train_test(np.reshape(5*np.random.randn(train_embeddings.size).astype(np.float32), train_embeddings.shape), \n",
    "                 np.reshape(5*np.random.randn(val_embeddings.size).astype(np.float32), val_embeddings.shape), \n",
    "                 np.reshape(5*np.random.randn(test_embeddings.size).astype(np.float32), test_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with random musiCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_V(nn.Module):\n",
    "    # vertical convolution\n",
    "    def __init__(self, input_channels, output_channels, filter_shape):\n",
    "        super(Conv_V, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, filter_shape,\n",
    "                              padding=(0, filter_shape[1]//2))\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        freq = x.size(2)\n",
    "        out = nn.MaxPool2d((freq, 1), stride=(freq, 1))(x)\n",
    "        out = out.squeeze(2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv_H(nn.Module):\n",
    "    # horizontal convolution\n",
    "    def __init__(self, input_channels, output_channels, filter_length):\n",
    "        super(Conv_H, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, filter_length,\n",
    "                              padding=filter_length//2)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        freq = x.size(2)\n",
    "        out = nn.AvgPool2d((freq, 1), stride=(freq, 1))(x)\n",
    "        out = out.squeeze(2)\n",
    "        out = self.relu(self.bn(self.conv(out)))\n",
    "        return out\n",
    "\n",
    "class Conv_1d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, stride=1, pooling=2):\n",
    "        super(Conv_1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, shape, stride=stride, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool1d(pooling)\n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "    \n",
    "class Musicnn(nn.Module):\n",
    "    '''\n",
    "    Pons et al. 2017\n",
    "    End-to-end learning for music audio tagging at scale.\n",
    "    This is the updated implementation of the original paper. Referred to the Musicnn code.\n",
    "    https://github.com/jordipons/musicnn\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                sample_rate=16000,\n",
    "                n_fft=512,\n",
    "                f_min=0.0,\n",
    "                f_max=8000.0,\n",
    "                n_mels=96,\n",
    "                n_class=50,\n",
    "                dataset='mtat'):\n",
    "        super(Musicnn, self).__init__()\n",
    "\n",
    "        # Spectrogram\n",
    "        self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                                         n_fft=n_fft,\n",
    "                                                         f_min=f_min,\n",
    "                                                         f_max=f_max,\n",
    "                                                         n_mels=n_mels)\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.spec_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # Pons front-end\n",
    "        m1 = Conv_V(1, 204, (int(0.7*96), 7))\n",
    "        m2 = Conv_V(1, 204, (int(0.4*96), 7))\n",
    "        m3 = Conv_H(1, 51, 129)\n",
    "        m4 = Conv_H(1, 51, 65)\n",
    "        m5 = Conv_H(1, 51, 33)\n",
    "        self.layers = nn.ModuleList([m1, m2, m3, m4, m5])\n",
    "\n",
    "        # Pons back-end\n",
    "        backend_channel= 512 if dataset=='msd' else 64\n",
    "        self.layer1 = Conv_1d(561, backend_channel, 7, 1, 1)\n",
    "        self.layer2 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "        self.layer3 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "\n",
    "        # Dense\n",
    "        dense_channel = 500 if dataset=='msd' else 200\n",
    "        self.dense1 = nn.Linear((561+(backend_channel*3))*2, dense_channel)\n",
    "        self.bn = nn.BatchNorm1d(dense_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(dense_channel, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Spectrogram\n",
    "        x = self.spec(x)\n",
    "        x = self.to_db(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.spec_bn(x)\n",
    "\n",
    "        # Pons front-end\n",
    "        out = []\n",
    "        for layer in self.layers:\n",
    "            out.append(layer(x))\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        # Pons back-end\n",
    "        length = out.size(2)\n",
    "        res1 = self.layer1(out)\n",
    "        res2 = self.layer2(res1) + res1\n",
    "        res3 = self.layer3(res2) + res2\n",
    "        out = torch.cat([out, res1, res2, res3], 1)\n",
    "\n",
    "        mp = nn.MaxPool1d(length)(out)\n",
    "        avgp = nn.AvgPool1d(length)(out)\n",
    "        out = torch.cat([mp, avgp], dim=1)\n",
    "        out = out.squeeze(2)\n",
    "\n",
    "        out = self.relu(self.bn(self.dense1(out)))\n",
    "        out = self.dropout(out)\n",
    "        #out = self.dense2(out)\n",
    "        #out = nn.Sigmoid()(out)\n",
    "\n",
    "        return out\n",
    "rand_musiCNN = Musicnn()\n",
    "rand_musiCNN.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store the embeddings for each subset\n",
    "if not os.path.isfile('train_embeddings_random.npy'):\n",
    "    i=0\n",
    "    train_embeddings = []\n",
    "    for track in train_dataset:\n",
    "        if i%100==0:\n",
    "            i+=1\n",
    "        print('Processing track '+str(i)+' of '+str(len(train_dataset)))\n",
    "        train_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    train_embeddings = np.array(train_embeddings)\n",
    "    np.save('train_embeddings_random.npy', train_embeddings)\n",
    "    \n",
    "    val_embeddings = []\n",
    "    for track in val_dataset:\n",
    "        val_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    val_embeddings = np.array(val_embeddings)\n",
    "    np.save('val_embeddings_random.npy', val_embeddings)\n",
    "    \n",
    "    test_embeddings = []\n",
    "    for track in test_dataset:\n",
    "        test_embeddings.append(rand_musiCNN(torch.from_numpy(resample(track[0].numpy()[0])).unsqueeze(0)).detach().numpy())\n",
    "    test_embeddings = np.array(test_embeddings)\n",
    "    np.save('test_embeddings_random.npy', test_embeddings)    \n",
    "\n",
    "else:\n",
    "    train_embeddings=np.load('train_embeddings_random.npy')\n",
    "    val_embeddings=np.load('val_embeddings_random.npy')\n",
    "    test_embeddings=np.load('test_embeddings_random.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_net = train_test(train_embeddings, val_embeddings, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model        | Loss           | Accuracy  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Random embeddings      | 2.31 | 8.07% |\n",
    "| Random musiCNN      | 1.88 | 40,58% |\n",
    "| musiCNN waveform      |    0.67   |   80.50% |\n",
    "| musiCNN melonMEL | 0.92      |    74.57% |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melon_venv",
   "language": "python",
   "name": "melon_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
